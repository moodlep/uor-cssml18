{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CSMML18 Homework1.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "private_outputs": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/moodlep/uor-cssml18/blob/master/CSMML18_Homework1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "ngLarV_a6F7l",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Vector and Matrix Calculus\n",
        "\n",
        "Let $w = [w_1 , w_2 , w_3 ]^T$ . \\\\\n",
        "Represent the following functions in the form of $w^T Aw$. \\\\\n",
        "(i) $g(w) = 5w_1^2 + w_2^2 + 5w_3^2 + 4w_1w_2 − 8w_1w_3 − 4w_2w_3$ \\\\\n",
        "(ii) $g(w) = 3w_1^2 + w_2^2 + 5w_3^2 + 4w_1w_2 − 6w_1w_3 − 4w_2w_3$ \\\\\n",
        "Find the Hessian of g(w). \n",
        "\n",
        "Approach: \\\\\n",
        "* Recall  that we know the Hessian in this very convenient matrix form below: \\\\\n",
        "$\\frac{\\partial ^2 w^TAw}{\\partial w^2} = A + A^T$ \\\\\n",
        "\n",
        "* To be able to use this form we need to represent $g(w)$ as $w^T Aw$ so we have A, then the hessian is easy to calc.\n"
      ]
    },
    {
      "metadata": {
        "id": "2XDQwExL66HB",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Concepts: \\\\\n",
        "\n",
        "* Multi-variate systems\n",
        "* Differentiation\n",
        "* Dot products\n",
        "* Hessians and Jacobians\n",
        "* Other extremely useful concepts in linear algebra for Machine Learning - see reference"
      ]
    },
    {
      "metadata": {
        "id": "5aZ8vfiyGJac",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Useful intuitions on [multi-variate calculus](https://www.khanacademy.org/math/multivariable-calculus/applications-of-multivariable-derivatives) from Khan Academy. \n",
        "\n",
        "This [article](https://www.khanacademy.org/math/multivariable-calculus/applications-of-multivariable-derivatives/modal/a/quadratic-approximation) explains why we seek the Hessian in quadratic form: \\\\\n",
        "$ax^2 + 2bxy + cy^2$ is represented in matrix form as \\\\\n",
        "$z^TMz$ where $z = [x,y]$ \\\\\n",
        "\n",
        "Note: Khan Academy has some *very nice videos* on multi-variate calculus. "
      ]
    },
    {
      "metadata": {
        "id": "-mQ-5xug6Diu",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Lagrange \n",
        "\n",
        "Let $w = [w_1, w_2]^T$ . \\\\\n",
        "$J(w) = 8w_1^2 + 7w_2 + 2w_1w_2 $ \\\\\n",
        "Use Lagrange method to find the minimum of $J(w)$, subject to $h(w) = 2w_1 +w_2 −2 = 0$\n",
        "\n",
        "Approach: \\\\\n",
        "* Identify you cost function $J(w)$ and your constraint function $h(w)$. \n",
        "* Write out the Lagrange function: $L(w,\\lambda) = J(w) + \\lambda h(w)$ \\\\\n",
        "(note this is the form for a single constraint - make sure you know what to do for multiple constraints, $h_i(w)$)\n",
        "* Recall we need to find  $\\nabla L(w,\\lambda) = 0$ and solve for $w=[w_1,w_2]$ and $\\lambda$ \n",
        "* Expand and solve: \n",
        "\n",
        "$\\frac{\\partial L}{\\partial w_1} = 0$ \\\\\n",
        "$\\frac{\\partial L}{\\partial w_2} = 0$ \\\\\n",
        "$\\frac{\\partial L}{\\partial \\lambda} = 0$ \\\\\n",
        "\n",
        "With 3 equations and 3 unknowns solve for $w$ and $\\lambda$ \\\\\n",
        "Tip: be clear in your solution. It is easy to make mistakes but if your method is clear you may get some marks. \n",
        "\n",
        "Concepts: \\\\\n",
        "\n",
        "* Optimisation\n",
        "* With constraints\n",
        "* Solving linear systems of equations\n"
      ]
    },
    {
      "metadata": {
        "id": "M_D8VSMwIo6t",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Parzens - Density Estimation"
      ]
    },
    {
      "metadata": {
        "id": "whMq6jz-Ew5y",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "#c=np.array([2, 2.5, 3, 1, 6])\n",
        "#c=np.array([-1, -1, 0, 0, 1])\n",
        "c=np.array([1, 2,3,4,5])\n",
        "#c=np.array([-1, 0.5, -1, 0, 0.5])\n",
        "x = np.zeros((1000))\n",
        "p1 = np.zeros((1000))\n",
        "p2 = np.zeros((1000))\n",
        "p3 = np.zeros((1000))\n",
        "p4 = np.zeros((1000))\n",
        "p5 = np.zeros((1000))\n",
        "p5 = np.zeros((1000))\n",
        "\n",
        "\n",
        "for i in range(1000):\n",
        "  x[i]=(i-500)/50\n",
        "  p1[i]=np.exp(-(x[i]-c[0])**2/2)/np.sqrt(2*np.pi)\n",
        "  p2[i]=np.exp(-(x[i]-c[1])**2/2)/np.sqrt(2*np.pi)\n",
        "  p3[i]=np.exp(-(x[i]-c[2])**2/2)/np.sqrt(2*np.pi)\n",
        "  p4[i]=np.exp(-(x[i]-c[3])**2/2)/np.sqrt(2*np.pi)\n",
        "  p5[i]=np.exp(-(x[i]-c[4])**2/2)/np.sqrt(2*np.pi)\n",
        "\n",
        "p=(p1+p2+p3+p4+p5)/5"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "BuKdzFBAJ7D3",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# To look at some of the data we generated: \n",
        "print(x[600:605])\n",
        "print(p1[600:605])\n",
        "print(p[600:605])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "nTiuK8YGFglM",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Plot the contribution from each point and the combined parzen's estimate\n",
        "plt.plot(x,p1, linestyle='dashed')\n",
        "plt.plot(x,p2, linestyle='dashed')\n",
        "plt.plot(x,p3, linestyle='dashed')\n",
        "plt.plot(x,p4, linestyle='dashed')\n",
        "plt.plot(x,p5, linestyle='dashed')\n",
        "plt.plot(x,p, color='black')\n",
        "plt.legend(['P1', 'P2', 'P3', 'P4', 'P5', 'P'], loc='upper left')\n",
        "plt.xlabel('x')\n",
        "plt.ylabel('Prob')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ZrFpWYWLQ8cP",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## GMM: \n",
        "\n",
        "The following link can be run as a Colab notebook and explains GMMs very well with code you can run so instead of duplicating it [here](https://jakevdp.github.io/PythonDataScienceHandbook/05.12-gaussian-mixtures.html) is the link. \n"
      ]
    },
    {
      "metadata": {
        "id": "jR1g4AZCLriz",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Clustering\n",
        "\n",
        "### Nearest Neighbours\n",
        "\n",
        "Key thing to remember is the distance measure: \\\\\n",
        "$d(x,x_k) = (x_1-x_{k1})^2 + (x_2-x_{k2})^2$ where $x = [x_1, x_2]^T$ and $x_k =[x_{k1}, x_{k2}]^T $\n",
        "\n",
        "Calculate distance of new point and existing points. Choose the smallest distance. \n",
        "\n",
        "### k-NN \n",
        "\n",
        "Same as above but comparison against new point and only k-closest points. \n",
        "\n",
        "### k-means\n",
        "\n",
        "Clusters have centroids. \n",
        "Online k-means update: $c_k^{new} = c_k^{old} + \\eta(x_j-c_k^{old})$"
      ]
    },
    {
      "metadata": {
        "id": "MtpGHkKI8zKI",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# General Resources: \n",
        "\n",
        "## ML Theory: \n",
        "\n",
        "* Nice high level overview of ML: https://homes.cs.washington.edu/~pedrod/papers/cacm12.pdf\n",
        "* Free copy of Bishop: https://www.microsoft.com/en-us/research/people/cmbishop/#!prml-book\n",
        "* Intro to ML - Andrew Ng: https://www.coursera.org/learn/machine-learning\n",
        "\n",
        "## Mathematics for ML: \n",
        "* https://mml-book.github.io/\n",
        "* https://www.coursera.org/specializations/mathematics-machine-learning\n",
        "\n",
        "## Probability and Stats\n",
        "* An Introduction to Statistical Learning: http://www-bcf.usc.edu/~gareth/ISL/\n",
        "* Khan Academy\n",
        "\n",
        "## Linear Algebra\n",
        "* Recommend you also work through the book by Strang https://www.amazon.co.uk/Introduction-Linear-Algebra-Gilbert-Strang/dp/0980232775/ref=dp_ob_title_bk \\\\\n",
        "With video lectures here: \\\\\n",
        "https://ocw.mit.edu/courses/mathematics/18-06-linear-algebra-spring-2010/video-lectures/\n",
        "* Khan Academy: https://www.khanacademy.org/math/multivariable-calculus\n"
      ]
    }
  ]
}