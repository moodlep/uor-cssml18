{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CSMML18 Homework1.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "private_outputs": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "ngLarV_a6F7l",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Vector and Matrix Calculus\n",
        "\n",
        "Let $w = [w_1 , w_2 , w_3 ]^T$ . \\\\\n",
        "Represent the following functions in the form of $w^T Aw$. \\\\\n",
        "(i) $g(w) = 5w_{12} + w_2 + 5w_{32} + 4w_1w_2 − 8w_1w_3 − 4w_2w_3$ \\\\\n",
        "(ii) $g(w) = 3w_{12} + w_2 + 5w_{32} + 4w_1w_2 − 6w_1w_3 − 4w_2w_3$ \\\\\n",
        "Find the Hessian of g(w). \n",
        "\n",
        "Approach: \\\\\n",
        "* extract A from $g(w)$\n",
        "* Then recall : \\\\\n",
        "$\\frac{\\partial ^2 w^TAw}{\\partial w^2} = A + A^T$"
      ]
    },
    {
      "metadata": {
        "id": "2XDQwExL66HB",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Concepts: \\\\\n",
        "\n",
        "* Multi-variate systems\n",
        "* Differentiation\n",
        "* Dot products\n",
        "* Hessians and Jacobians\n",
        "* Other extremely useful concepts in linear algebra for Machine Learning - see reference"
      ]
    },
    {
      "metadata": {
        "id": "5aZ8vfiyGJac",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Useful intuitions on [multi-variate calculus](https://www.khanacademy.org/math/multivariable-calculus/applications-of-multivariable-derivatives) from Khan Academy. \n",
        "\n",
        "This [article](https://www.khanacademy.org/math/multivariable-calculus/applications-of-multivariable-derivatives/modal/a/quadratic-approximation) explains why we seek the Hessian in quadratic form: \\\\\n",
        "$ax^2 + 2bxy + cy^2$ is represented in matrix form as \\\\\n",
        "$z^TMz$ where $z = [x,y]$"
      ]
    },
    {
      "metadata": {
        "id": "-mQ-5xug6Diu",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Lagrange \n",
        "\n",
        "Let $w = [w1, w2]^T$ . \\\\\n",
        "$J(w) = 8w12 + 7w2 + 2w1w2 $ \\\\\n",
        "Use Lagrange method to find the minimum of $J(w)$, subject to $h(w) = 2w1 +w2 −2 = 0$\n",
        "\n",
        "Concepts: \\\\\n",
        "\n",
        "* Optimisation\n",
        "* With constraints\n",
        "* Solving linear systems of equations\n"
      ]
    },
    {
      "metadata": {
        "id": "M_D8VSMwIo6t",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Parzens - Density Estimation"
      ]
    },
    {
      "metadata": {
        "id": "whMq6jz-Ew5y",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "#c=np.array([2, 2.5, 3, 1, 6])\n",
        "c=np.array([-1, -1, 0, 0, 1])\n",
        "#c=np.array([1, 2,3,4,5])\n",
        "#c=np.array([-1, 0.5, -1, 0, 0.5])\n",
        "x = np.zeros((1000))\n",
        "p1 = np.zeros((1000))\n",
        "p2 = np.zeros((1000))\n",
        "p3 = np.zeros((1000))\n",
        "p4 = np.zeros((1000))\n",
        "p5 = np.zeros((1000))\n",
        "p5 = np.zeros((1000))\n",
        "\n",
        "\n",
        "for i in range(1000):\n",
        "  x[i]=(i-500)/50\n",
        "  p1[i]=np.exp(-(x[i]-c[0])**2/2)/np.sqrt(2*np.pi)\n",
        "  p2[i]=np.exp(-(x[i]-c[1])**2/2)/np.sqrt(2*np.pi)\n",
        "  p3[i]=np.exp(-(x[i]-c[2])**2/2)/np.sqrt(2*np.pi)\n",
        "  p4[i]=np.exp(-(x[i]-c[3])**2/2)/np.sqrt(2*np.pi)\n",
        "  p5[i]=np.exp(-(x[i]-c[4])**2/2)/np.sqrt(2*np.pi)\n",
        "\n",
        "p=(p1+p2+p3+p4+p5)/5"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "nTiuK8YGFglM",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "plt.plot(x,p1, linestyle='dashed')\n",
        "plt.plot(x,p2, linestyle='dashed')\n",
        "plt.plot(x,p3, linestyle='dashed')\n",
        "plt.plot(x,p4, linestyle='dashed')\n",
        "plt.plot(x,p5, linestyle='dashed')\n",
        "plt.plot(x,p, color='black')\n",
        "plt.legend(['P1', 'P2', 'P3', 'P4', 'P5', 'P'], loc='upper left')\n",
        "plt.xlabel('x')\n",
        "plt.ylabel('Prob')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "BuKdzFBAJ7D3",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "print(x[600:605])\n",
        "print(p1[600:605])\n",
        "print(p[600:605])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "jR1g4AZCLriz",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Clustering\n",
        "\n",
        "### Nearest Neighbours\n",
        "\n",
        "Key thing to remember is the distance measure: \\\\\n",
        "$d(x,x_k) = (x_1-x_{k1})^2 + (x_2-x_{k2})^2$ where $x = [x1, x2]^T$ and $x_k =[x_{k1}, x_{k2}]^T $\n",
        "\n",
        "Calculate distance of new point and existing points. Choose the smallest distance. \n",
        "\n",
        "### k-NN \n",
        "\n",
        "Same as above but comparison against new point and only k-closest points. \n",
        "\n",
        "### k-means\n",
        "\n",
        "Clusters have centroids. \n",
        "Online k-means update: $c_k^{new} = c_k^{old} + \\eta(x_j-c_k^{old})$"
      ]
    },
    {
      "metadata": {
        "id": "MtpGHkKI8zKI",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# General Resources: \n",
        "\n",
        "## ML Theory: \n",
        "\n",
        "* Free copy of Bishop: https://www.microsoft.com/en-us/research/people/cmbishop/#!prml-book\n",
        "* Intro to ML - Andrew Ng: https://www.coursera.org/learn/machine-learning\n",
        "\n",
        "## Mathematics for ML: \n",
        "* https://mml-book.github.io/\n",
        "* https://www.coursera.org/specializations/mathematics-machine-learning\n",
        "\n",
        "## Probability and Stats\n",
        "* An Introduction to Statistical Learning: http://www-bcf.usc.edu/~gareth/ISL/\n",
        "* Khan Academy\n",
        "\n",
        "## Linear Algebra\n",
        "* Recommend you also work through the book by Strang https://www.amazon.co.uk/Introduction-Linear-Algebra-Gilbert-Strang/dp/0980232775/ref=dp_ob_title_bk \\\\\n",
        "With video lectures here: \\\\\n",
        "https://ocw.mit.edu/courses/mathematics/18-06-linear-algebra-spring-2010/video-lectures/\n",
        "* Khan Academy: https://www.khanacademy.org/math/multivariable-calculus\n"
      ]
    }
  ]
}