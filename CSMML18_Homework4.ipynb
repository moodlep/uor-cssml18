{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CSMML18 Homework4.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "private_outputs": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/moodlep/uor-cssml18/blob/master/CSMML18_Homework4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "UZqJF9_Pbch_",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## KKT\n",
        "\n",
        "Minimise $f(x)$ subject to $h(x) = 0, g(x) ≤ 0$\n",
        "\n",
        "\n",
        "### Q2 Expanded\n",
        "Find the solution of min $f(x, y) = x^2 + y − 1$ , subject to $x + y ≥ 5$.\n",
        "\n",
        "####Approach\n",
        "\n",
        "* Write the problem out in the standard form \n",
        "\n",
        "min $f(x, y) = x^2 + y − 1$ , s.t. $g(x,y) = -x - y +5 ≤ 0$.\n",
        "\n",
        "1 - $\\mu ≥ 0$  we only have one constraint\n",
        "\n",
        "2 - $Df + \\mu Dg = 0 $\n",
        "\n",
        "3 - $\\mu g = 0$\n",
        "\n",
        "* Expand the derivative terms\n",
        "\n",
        "$\\frac{\\partial f}{\\partial x} + \\mu \\frac{\\partial g}{\\partial x} = 0 = 2x - \\mu$ \\\\\n",
        "\n",
        "$\\frac{\\partial f}{\\partial y} + \\mu \\frac{\\partial g}{\\partial y} = 0 = 1-\\mu $ which implies $\\mu = 1$ \\\\\n",
        "\n",
        "* Solve and test the various options. \n",
        "Subst. $\\mu$ back into the equations above and solve for the other parameters. Test if all values hold. "
      ]
    },
    {
      "metadata": {
        "id": "eBN8JEML16dv",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Kernels\n",
        "\n",
        "Refer to Bishop, [Section 6](https://www.microsoft.com/en-us/research/uploads/prod/2006/01/Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf) for some very clear explanations. \n",
        "\n",
        "###Q4\n",
        "Supposing $x = [x_1, x_2]^T, y = [y_1, y_2]^T$ , determine the feature map for $K(x, y) = (x^Ty + 1)^2$.\n",
        "\n",
        "Approach: \n",
        "\n",
        "* Expand $K(x, y) = (x^Ty + 1)^2$ \n",
        "\n",
        "$K(x, y) = (x^Ty + 1)^2 = ([x_1 x_2]^T[y_1 y_2] + 1)^2 = x_1^2y_1^2 + x_2^2y_2^2 + 2x_1x_2y_1y_2 + 2x_1y_1 + 2x_2y_2 + 1$\n",
        "\n",
        "* Separate out the $x$ and $y$ terms\n",
        "\n",
        "* Compose basis function $\\phi(x)$ and $\\phi(y)$ such that $<\\phi(x), \\phi(y)>$ reproduces the expression above. \\\\\n",
        "$\\phi(x) = [x_1^2, x_2^2, \\sqrt{2} x_1x_2, \\sqrt{2}x_1, \\sqrt{2}x_2, 1]$ \\\\\n",
        "$\\phi(y) = [y_1^2, y_2^2, \\sqrt{2} y_1y_2, \\sqrt{2}y_1, \\sqrt{2}y_2, 1]$\n"
      ]
    },
    {
      "metadata": {
        "id": "kw06WEZsivdo",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "###Q5 \n",
        "\n",
        "Determine if $K(x, y) = (x^Ty + 1)^2 + exp(−∥x − y∥^2)$ is a kernel\n",
        "\n",
        "We know $K(x, y) = (x^Ty + 1)^2 $ is a kernel and that kernels are modular so just need to show that $exp(−∥x − y∥^2)$ is a kernel. \n",
        "\n",
        "Refer to Bishop: Page 296 for rules related to kernels and the proof below is on page 297. For now we use\n",
        "\n",
        "$k(x,x') = f(x)k_1(x,x')f(x')$  [Ref Bishop page 296, eqn 6.14] \\\\\n",
        "$k(x,x') = exp (k_1 (x, x' ))$  [Ref Bishop page 296, eqn 6.16] \\\\\n",
        "Where $k_1(x,x')$ is a kernel. \\\\\n",
        "\n",
        "Starting with $−∥x − y∥^2$, expand to $x^Tx -2x^Ty + y^Ty$ \\\\\n",
        "\n",
        "Subst. into the original \n",
        "\n",
        "$k(x,y) = exp(−∥x − y∥^2) = exp(-x^Tx) exp(2x^Ty) exp(-y^Ty) $ \n",
        "\n",
        "Where $exp(2x^Ty)$ is a linear kernel (using eqn. 6.16) and \n",
        "\n",
        "$k(x,y)$ is in the form of eqn 6.14 above which implies it is a kernel. \n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "tGkOg8c29dhF",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "###Q6 \n",
        "\n",
        "Consider two class data set: \n",
        "$$ \\begin{bmatrix}\n",
        "x_1 & x_2 & t \\\\ \n",
        "-1 & -1 &  1 \\\\\n",
        "-1 & 1 &-1 \\\\\n",
        "1 & -1 & -1 \\\\\n",
        "1 & 1 & 1\n",
        "\\end{bmatrix}$$\n",
        "What is the kernel matrix if $K(x, y) = (x^Ty + 1)^2 + exp(−∥x − y∥^2) $?\n",
        "\n",
        "####Approach: \n",
        "\n",
        "* Given 4 data points so the kernel matrix is going to be $4 \\times 4 $\n",
        "\n",
        "* Say $K(x, y) = K_1(x,y) + K_2(x,y)$ where $K_1(x, y) = (x^Ty + 1)^2$ and $K_2(x, y) = exp(−∥x − y∥^2) $, first find $K_1(x,y)$\n",
        "\n",
        "Recall the Kernel matrix is symmetric so not all positions need to be calculated. \n",
        "\n",
        "Example on the diagonal: $K_1[1,1]$ or $row 1, col 1 = (\\begin{bmatrix}-1 & -1 \\end{bmatrix} \\begin{bmatrix}-1 \\\\ -1 \\end{bmatrix} + 1)^2 = (3)^2 = 9$\n",
        "\n",
        "Example off the diagonal: $K_1[1,2]$ or $row 1, col 2 = (\\begin{bmatrix}-1 & -1 \\end{bmatrix} \\begin{bmatrix}-1 \\\\ 1 \\end{bmatrix} + 1)^2 = (1)^2 = 1$\n",
        "\n",
        "* Do the same for $K_2(x, y) = exp(−∥x − y∥^2) $\n",
        "\n",
        "Example on the diagonal: $K_2[1,1]$ or $row 1, col 1 = exp(-((-1+1)^2 + (-1+1)^2)) = 1$\n",
        "\n",
        "Example off the diagonal: $K_2[1,2]$ or $row 1, col 2 = exp(-((-1+1)^2 + (-1-1)^2)) = 0.0183$\n",
        "\n",
        "* Finally sum the kernel matrices: \n",
        "\n",
        "Example on the diagonal: $K[1,1] = K_1[1,1] + K_2[1,1] = 9+1 = 10$\n",
        "\n",
        "Example off the diagonal: $K[1,2] = K_1[1,2] + K_2[1,2] = 1 +  0.0183 = 1.0183$"
      ]
    },
    {
      "metadata": {
        "id": "KGipbqUaDfpC",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "###Q7 \n",
        "For the data set in (6), what is the objective function of support vector machine is dual form in the cases that the kernel is \n",
        "$K(x, y) = (x^Ty + 1)^2$ or  $K(x,y) = exp(−∥x − y∥^2) $ respectively?\n",
        "\n",
        "####Approach: \n",
        "\n",
        "* Write out the objective function in dual form: \n",
        "\n",
        "$L(a) = \\sum_{n=1}^N a_n - \\frac{1}{2} \\sum_{n=1}^N \\sum_{m=1}^N a_na_mt_nt_mk(x_n,x_m)$\n",
        "\n",
        "* Substitute the values from the Kernel matrices in to form a function of $a$, for example using $K(x, y) = (x^Ty + 1)^2$ \n",
        "\n",
        "$L(a) = a_1 + a_2 + a_3 + a_4 - \\frac{1}{2} ( $ \\\\\n",
        "$a_1a_1(1)(1)(9) + a_1a_2(1)(-1)(1) + a_1a_3(1)(-1)(1) + a_1a_4(1)(1)(1) $ \\\\\n",
        "$a_2a_1(-1)(1)(1) + a_2a_2(-1)(-1)(9) + a_2a_3(-1)(-1)(1) + a_2a_4(-1)(1)(1) $ \\\\\n",
        "$a_3a_1(-1)(1)(1) + a_3a_2(-1)(-1)(1) + a_3a_3(-1)(-1)(9) + a_3a_4(-1)(1)(1) $ \\\\\n",
        "$a_4a_1(1)(1)(1) + a_4a_2(1)(-1)(1) + a_4a_3(1)(-1)(1) + a_4a_4(1)(1)(9) )$ \\\\\n",
        "\n",
        "$ = a_1 + a_2 + a_3 + a_4 - \\frac{1}{2} ( 9a_1^2 -2a_1a_2 -2a_1a_3 +2a_1a_4 + 9a_2^2 +2a_2a_3 -2a_2a_4 + 9a_3^2 -2a_3a_4 + 9a_4^2 )$\n",
        "\n",
        "* Repeat for the second kernel. "
      ]
    },
    {
      "metadata": {
        "id": "tBDJhQStkM8j",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "###Kernels - Some points:\n",
        "\n",
        "* The kernel is also known as a similarity function\n",
        "* The kernel is an inner product between two input feature vectors. The kernel matrix therefore contains a scalar representation of the input feature vectors. This allows for a dual representation using the kernels instead of the input vectors. \n",
        "* The kernel trick is based on being able to substitute kernels when the dual representation is available. \n",
        "* An example of how this works in SVM using first a linear and then a non-linear dataset is available [here](https://jakevdp.github.io/PythonDataScienceHandbook/05.07-support-vector-machines.html). Run through the colab. \n",
        "* Some intuitive explanations of how the SVM algorithm is developed in various ways is [here](https://mml-book.github.io/book/chapter12.pdf), as reviewed in the tutorial. In general this is a really good book to start with for ML.  \n"
      ]
    },
    {
      "metadata": {
        "id": "aoY_BTUXHbj0",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Resources\n",
        "\n",
        "* Kernels - a basic [overview](https://www.cs.princeton.edu/~bee/courses/scribe/lec_10_09_2013.pdf) \n",
        "* Kernels - a slightly more complete [overview](http://www.gatsby.ucl.ac.uk/~gretton/coursefiles/lecture4_introToRKHS.pdf). \n",
        "\n",
        "### Supplementary courses at UoR: \n",
        "\n",
        "* All course [Modules](http://www.reading.ac.uk/modules/module.aspx?sacyr=1819&school=MPS) for Math, Meteorology, CS\n",
        "* [Probability Theory I](http://www.reading.ac.uk/modules/document.aspx?modP=MA2PT1&modYR=1819)\n",
        "* [Data Science and ML](http://www.reading.ac.uk/modules/document.aspx?modP=ST3SML&modYR=1819)\n"
      ]
    }
  ]
}