{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CSSML18 Homework3.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "private_outputs": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/moodlep/uor-cssml18/blob/master/CSSML18_Homework3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "QvRB-DU9t4JS",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Neural Networks and Gradient Descent\n",
        "\n",
        "Main concepts: \n",
        "\n",
        "* $y(x,w) = h(w^Tx)$   -> know how to draw the network from a description. \n",
        "\n",
        "A useful simplification to make: $ v = w^Tx$, so $y(x,w) = h(v)$\n",
        "\n",
        "Where $h(v)$  is the activation function. \n",
        "* $E(w) = \\frac{1}{2} \\sum_{n=1}^{N} || y_n - t_n || ^2 $  \n",
        "\n",
        "* Updating the weights: $w^{new} = w^{old} - \\eta \\frac{dE}{dw}$\n",
        "\n",
        "* Where $\\frac{dE}{dw} = (y_n-t_n)\\frac{dy}{dw}$ \n",
        "\n",
        "* Recall that $\\bbox[5px,border:1px solid black]{\n",
        "\\frac{dy}{dw} = y' =  \\frac{dh}{dw} = \\frac{dh}{dv}\\frac{dv}{dw}\n",
        "}$\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "PEDuvJ1xt6ft",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Q2\n",
        "\n",
        "The online gradient descent algorithm is used to train a single layer perceptron given by \n",
        "\n",
        "$y(x, w) = h(∑^2_{i=0}w_ix_i) $ where $x ∈ [x_1, x_2]^T $ and $x_0 = 1, w = [w_0, w_1, w_2]$\n",
        "\n",
        "$h(v) = tanh(v) = \\frac{exp(v)−exp(−v)} {exp(v)+exp(−v)} $\n",
        "\n",
        "(Note: $h'(v) = (1 − h(v)^2)$). \n",
        "\n",
        "Assume that the current weight vector is $w = [1, 0.3, 0.4]^T$, Calculate the new weight updated from a new training datum\n",
        "$[x, t] = [2, 1, 0.2]^T$, using the learning rate $η = 0.02$.\n",
        "\n",
        "\n",
        "####Approach: \n",
        "\n",
        "\n",
        "To update the weights: $w^{new} = w^{old} - \\eta \\frac{dE}{dw}$ \n",
        "\n",
        "* We have $w^{old} =  [1, 0.3, 0.4]^T$, $ \\eta = 0.02$, $x =  [1, 2, 1]^T$ and $t_n = 0.2$. \n",
        "\n",
        "\n",
        "* We need $\\frac{dE}{dw} = (y_n-t_n)\\frac{dy}{dw}$  where $\\frac{dy}{dw} = y' =  \\frac{dh}{dw} = \\frac{dh}{dv}\\frac{dv}{dw}$\n",
        "\n",
        "\n",
        "* We can calculate $ y_n = h(v), v = w^Tx $. \n",
        "\n",
        "\n",
        "* For $y' =  \\frac{dh}{dw} = \\frac{dh}{dv}\\frac{dv}{dw}$ we have $\\frac{dh}{dv} = h'(v) = (1 − h(v)^2)$ so we still need $\\frac{dv}{dw} = x^T$\n",
        "\n",
        "\n",
        "** NOTE ** this last term was omitted by mistake during the class!! Let me know if we need to review this in the next class. \n",
        "\n",
        "\n",
        "* Pulling all the terms together we get \\\\\n",
        "$\\frac{dE}{dw} = (y_n-t_n)(1 − y_n^2)[1,  x_n^T]^T  $ \\\\\n",
        "where $[1, x^T]$ is equivalent to adding the $x_0$ component to the vector $x$, for example $x = [1, 2, 1]^T$\n"
      ]
    }
  ]
}